{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "\n",
    "Classifiers are algorithms that given two or more groups of samples, find the features that distinguish them the most. Different algorithms define \"the most\" differently, and we'll look into two of them.\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "Before we get started with using these algorithms, we're going to talk about overfitting. \n",
    "\n",
    "In classical machine learning, overfitting is when you've fit your model so perfectly to your data that it's not generalizable to other cases. For example, you designed a cancer vs normal classifier algorithm that works perfectly in a single breast cancer dataset but when you try it on a new breast cancer dataset, it doesn't work.\n",
    "\n",
    "Overfitting is a common newbie mistake that leads to bad things because not all of the \"significant\" features you found are actually useful - they randomly were higher or lower in your training data and thus the algorithm thought they were significant.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/1/19/Overfitting.svg)\n",
    "Source: http://scott.fortmann-roe.com/docs/MeasuringError.html\n",
    "\n",
    "### How to avoid overfitting\n",
    "\n",
    "In machine learning, you often have a \"gold standard\" dataset from which you're learning the data (e.g. a manually curated face recognition dataset) and then a separate dataset that you test on. In biology, we tend to not have a good manually curated dataset for RNA-seq (though that would be very nice!!!), so what we do instead is split up our dataset to testing and training sets.\n",
    "\n",
    "#### Training and test datasets\n",
    "\n",
    "![](figures/test_train_split_holdout.png)\n",
    "\n",
    "\n",
    "#### Rerunning classifiers\n",
    "\n",
    "Another method you can use to prevent overfitting is rerunning your data multiple times on the same classifier, using different random seeds. A \"random seed\" is a starting point for random number generators. The code below uses the random number generator in Python to return a random integer from 0 to 100. \n",
    "\n",
    "1. Run the code below a few times to see different random numbers from 0 to 100.\n",
    "2. Uncomment (remove the \"#\" hash mark at the beginning of the line)\n",
    "3. Try changing the random seed (the number between the parentheses of `random.seed()` - right now it is 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sets the random seed\n",
    "#random.seed(9)\n",
    "\n",
    "# Return a random integer from 0 to 100\n",
    "random.randint(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this matter to classifiers? Many times, they will pick a random set of features as an initial guess at the best discriminating features, and sometimes it gets lucky and sometimes it doesn't. You want to make sure that your classification features are robust to multiple different initializations.\n",
    "\n",
    "### [Quiz 3.1.1](https://docs.google.com/forms/d/1rMy_oXddUATlZPdhaqifkrFOLteT8p8eAxH1TUlSvjM/viewform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machines (SVM)\n",
    "\n",
    "Given a high dimensional dataset, SVMs\n",
    "\n",
    "### Pros\n",
    "\n",
    "* Simple and easy to train method\n",
    "* Does well agaisnt overfitting\n",
    "\n",
    "### Cons\n",
    "\n",
    "* Data must be linearly separable\n",
    "    * Solution: Use a de-circularizing projection of your data (using a radial basis function, RBF kernel)\n",
    "    * Still need to be able to draw an obvious line between your groups\n",
    "    \n",
    "    \n",
    "For this topic, we'll use a tutorial on SVM from Jake Vanderplas' excellent day-long [tutorial](https://github.com/jakevdp/sklearn_tutorial) on scikit-learn (`sklearn`) (which I highly recommend working through on your own), the machine learning library in Python. Follow [this link](../sklearn_tutorial/notebooks/03.1-Classification-SVMs.ipynb), stopping when you get to the section labeled **Quick Example: Moving to Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision trees\n",
    "\n",
    "Decision trees are classifiers which find the features whose cutoffs best define the differences between your groups. For example, below is an example decision tree of teaching a computer how to classify fruit from the grocery store.\n",
    "\n",
    "![](figures/decision_tree_fruit.png)\n",
    "Source: https://www.safaribooksonline.com/library/view/programming-collective-intelligence/9780596529321/ch12s02.html\n",
    "\n",
    "Again, for this topic, we'll use Jake's [tutorial](../sklearn_tutorial/notebooks/03.2-Regression-Forests.ipynb)\n",
    "\n",
    "### Pros\n",
    "\n",
    "* Agnostic to the distributions of your data\n",
    "* Doesn't need to be linearly separable\n",
    "\n",
    "### Cons\n",
    "\n",
    "* Can't deal with intermediate populations (e.g. $x < 10$ , $10\\geq x < 15$ , $15 \\geq x$ wouldn't get picked up)\n",
    "    * Solution: train one class at a time\n",
    "* ***Overfitting***\n",
    "    * False positives\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
