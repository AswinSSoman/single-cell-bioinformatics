{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import FastICA, PCA \n",
    "\n",
    "sns.set(style='white', context='notebook')\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix decomposition methods: PCA and ICA\n",
    "\n",
    "The two first methods we will talk about are true matrix decomposition methods. They are trying to decompose a matrix $X$ into constitutent parts, $Y$ and $W$.\n",
    "\n",
    "$Y = WX$\n",
    "\n",
    "These matrix equations may be kind of intimidating so one way to think about them adding the signal from genes:\n",
    "\n",
    "$\n",
    "\\text{Component }1 = 10\\text{gene}_1 - 50\\text{gene}_2 + 2\\text{gene}_3 \\ldots\n",
    "$\n",
    "\n",
    "Depending on the algorithm the coefficients will have different constraints (have to sum to one or be independent or something annoying like that) but the idea is the same: summarize the gene expression (features) into fewer components, each of which are linear combinations of the original genes (features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Conceptually speaking, PCA is a rotation of the data:\n",
    "\n",
    "![](figures/pca_unrotated.png)\n",
    "![](figures/pca_rotation.png)\n",
    "\n",
    "Principal component analysis, statistically speaking, finds the axes of most variance in the data. Put another way, PCA finds the directions of genes that change the most across your data, then the second most changing genes, then the next.\n",
    "\n",
    "PCA decomposes your data matrix with the constraint that the first component must \"explain\" the majority of the data. What does \"explain\" really mean here?\n",
    "\n",
    "![](figures/pca.png)\n",
    "\n",
    "Source: http://www.nlpca.org/pca_principal_component_analysis.html\n",
    "\n",
    "What \"explain\" means is that the first axis represents the direction of the data that varies the most. PCA assumes that low variance means noise. \n",
    "\n",
    "\n",
    "Let's take the first component first. The data is projected onto a line for the first component that minimizes the distance between points from the first component line:\n",
    "\n",
    "![](figures/pca_distance_from_line.jpg)\n",
    "\n",
    "source: https://liorpachter.wordpress.com/2014/05/26/what-is-principal-component-analysis/\n",
    "\n",
    "### Side note: Kernel PCA\n",
    "\n",
    "Kernel PCA is best used on non-linear data. Kernel PCA means that you did a \"kernel transformation\" - usually a [\"radial basis function\"](https://en.wikipedia.org/wiki/Radial_basis_function) on the data to force it to be linear beforehand:\n",
    "\n",
    "![](figures/plot_kernel_pca_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Component Analysis (ICA)\n",
    "\n",
    "ICA was originally created for the \"cocktail party problem\" for audio processing. It's an incredible feat that our brains are able to filter out all these different sources of audio, automatically!\n",
    "\n",
    "![](figures/Cocktail-party-_2502341b.jpg)\n",
    "(I really like how smug that guy looks - it's really over the top)\n",
    "[Source](http://www.telegraph.co.uk/news/science/science-news/9913518/Cocktail-party-problem-explained-how-the-brain-filters-out-unwanted-voices.html)\n",
    "\n",
    "### Cocktail party problem\n",
    "\n",
    "Given multiple sources of sound (people talking, the band playing, glasses clinking), how do you distinguish independent sources of sound? Imagine at a cocktail party you have multiple microphones stationed throughout, and you get to hear all of these different sounds. \n",
    "\n",
    "![](figures/independent-component-analysis-ica-the-cocktail-party-problem-n.jpg)\n",
    "\n",
    "[Source](http://www.slideserve.com/vladimir-kirkland/ica-and-isa-using-schweizer-wolff-measure-of-dependence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if you applied PCA to the cocktail party problem?\n",
    "\n",
    "What would you get if you \n",
    "\n",
    "Example adapted from the excellent [scikit-learn documentation](http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n_samples = 2000\n",
    "time = np.linspace(0, 8, n_samples)\n",
    "\n",
    "s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
    "s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n",
    "s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n",
    "\n",
    "S = np.c_[s1, s2, s3]\n",
    "S += 0.2 * np.random.normal(size=S.shape)  # Add noise\n",
    "\n",
    "S /= S.std(axis=0)  # Standardize data\n",
    "# Mix data\n",
    "A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\n",
    "X = np.dot(S, A.T)  # Generate observations\n",
    "\n",
    "# Compute ICA\n",
    "ica = FastICA(n_components=3)\n",
    "S_ = ica.fit_transform(X)  # Reconstruct signals\n",
    "A_ = ica.mixing_  # Get estimated mixing matrix\n",
    "\n",
    "# We can `prove` that the ICA model applies by reverting the unmixing.\n",
    "assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)\n",
    "\n",
    "# For comparison, compute PCA\n",
    "pca = PCA(n_components=3)\n",
    "H = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components\n",
    "\n",
    "###############################################################################\n",
    "# Plot results\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "models = [X, S, S_, H]\n",
    "names = ['Observations (mixed signal)',\n",
    "         'True Sources',\n",
    "         'ICA recovered signals', \n",
    "         'PCA recovered signals']\n",
    "colors = sns.color_palette('colorblind')\n",
    "\n",
    "for ii, (model, name) in enumerate(zip(models, names), 1):\n",
    "    plt.subplot(4, 1, ii)\n",
    "    plt.title(name)\n",
    "    for sig, color in zip(model.T, colors):\n",
    "        plt.plot(sig, color=color)\n",
    "\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)\n",
    "sns.despine()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 2.3.1\n",
    "\n",
    "[Link](https://docs.google.com/forms/d/1D3D1IE7q6cqpX7isW9-EIIDH5ixKkLOUzSlKyno1n2I/viewform)\n",
    "\n",
    "## PCA vs ICA\n",
    "\n",
    "Which one should you use? Well, that depends on your question :)\n",
    "\n",
    "PCA and ICA have different goals. PCA wants to find the things that change the greatest across your data, and ICA wants to find individual signals. Let's take a look at this by running both PCA and ICA on data that we're all familiar with - faces!\n",
    "\n",
    "The \"Olivetti Faces Dataset\" is a commonly use face recognition dataset in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.plot??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_components = 9\n",
    "\n",
    "\n",
    "n_row, n_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Authors: Vlad Niculae, Alexandre Gramfort\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import logging\n",
    "from time import time\n",
    "\n",
    "from numpy.random import RandomState\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close('all')\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "image_shape = (64, 64)\n",
    "rng = RandomState(0)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "###############################################################################\n",
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# local centering\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "def plot_gallery(title, images, n_col=5, n_row=5, cmap=plt.cm.viridis):\n",
    "    plt.figure(figsize=(2. * n_col/2, 2.26 * n_row/2))\n",
    "    plt.suptitle(title)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        if comp.min() < 0:\n",
    "            vmax = max(comp.max(), -comp.min())\n",
    "            vmin = -vmax\n",
    "        else:\n",
    "            vmin = comp.min()\n",
    "            vmax = comp.max()\n",
    "        plt.imshow(comp.reshape(image_shape), cmap=cmap,\n",
    "                   interpolation='nearest',\n",
    "                   vmin=vmin, vmax=vmax)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "###############################################################################\n",
    "# Plot a sample of the input data\n",
    "\n",
    "\n",
    "\n",
    "plot_gallery(\"First centered Olivetti faces\", faces[:25], cmap=plt.cm.gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first figure and its subpanels show the first 20 (out of 400) faces in the dataset. \n",
    "\n",
    "So now let's explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore_pca_ica(algorithm, n_components):\n",
    "    \n",
    "    # establish size of the figure to plot by the number\n",
    "    # of rows and columns of subplots\n",
    "    n_row = 1\n",
    "    n_col = 1\n",
    "    while n_row * n_col < n_components:\n",
    "        if n_col > n_row:\n",
    "            n_row += 1\n",
    "        else:\n",
    "            n_col += 1\n",
    "            \n",
    "    \n",
    "    kwargs = dict(whiten=True, n_components=n_components)\n",
    "    if algorithm == 'PCA':\n",
    "        decomposer = PCA(**kwargs)\n",
    "    elif algorithm == 'ICA':\n",
    "        kwargs['random_state'] = 2016\n",
    "        kwargs['max_iter'] = 200\n",
    "        kwargs['tol'] = 0.001\n",
    "        decomposer = FastICA(**kwargs)\n",
    "    \n",
    "    t0 = time()\n",
    "    decomposer.fit(X=faces_centered)\n",
    "    train_time = (time() - t0)\n",
    "    print(\"done in %0.3fs\" % train_time)\n",
    "    \n",
    "    plot_gallery('%s - Train time %.1fs' % (algorithm, train_time),\n",
    "                 decomposer.components_[:n_components], n_col=n_col, n_row=n_row)\n",
    "\n",
    "\n",
    "ipywidgets.interact(explore_pca_ica,\n",
    "                   algorithm=ipywidgets.Dropdown(options=['PCA', 'ICA'], value='PCA',\n",
    "                                                  description='Matrix decomposition algorithm'),\n",
    "                   n_components=ipywidgets.IntSlider(min=2, max=50, value=12));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows you the *components* of the data. \n",
    "\n",
    "Notice that in PCA, these are \"eigenfaces,\" that is, the first face is the most average face that explains most of the data. The next components shows where the next largest amount of variance is. As you continue, the components of PCA goes into the edge cases of the different faces so you can reconstruct more and more faces.\n",
    "\n",
    "For ICA, we don't get an \"eigenface.\" Instead, ICA goes right into the discrete signals. Notice that some of the ICA components actually look like an individual person's face, not an average of people's faces. ICA is pulling out individual people who had their photo taken multiple times in the dataset, and reconstructing them.\n",
    "\n",
    "### Quiz 2.3.2\n",
    "\n",
    "Work on this [quiz](https://docs.google.com/forms/d/1Tj2mwbOAMQlsEM_tx1ZtC4ahvbqgfjjoWXuwOOO788c/viewform) while you play with the sliders above.\n",
    "\n",
    "#### The punchline\n",
    "\n",
    "Which should you use, PCA or ICA? Again, it depends on your question!\n",
    "\n",
    "PCA tells you which are the largest varying genes in your data. ICA tells you which genes contribute to discrete signals from specific populations in your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
